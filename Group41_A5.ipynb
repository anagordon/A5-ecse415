{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8abef9e0",
      "metadata": {
        "id": "8abef9e0"
      },
      "source": [
        "# ECSE 415 - Assignment 5: Video Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d45b40",
      "metadata": {
        "id": "13d45b40"
      },
      "source": [
        "## Authors\n",
        "\n",
        "| *Name*                         |*Student ID*|\n",
        "|--------------------------------|------------|\n",
        "| Ana Gordon                     | 261113440 |\n",
        "| Mathias Nahuel Pacheco Lemina  | 261116679  |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54bc4034",
      "metadata": {
        "id": "54bc4034"
      },
      "source": [
        "## Introduction\n",
        "The goal of this assignment is "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jm0WARJnuYxY",
      "metadata": {
        "id": "Jm0WARJnuYxY"
      },
      "source": [
        "https://www.freecodecamp.org/news/how-to-detect-objects-in-images-using-yolov8/#heading-how-to-get-started-with-yolov8\n",
        "\n",
        "https://www.geeksforgeeks.org/python/saving-a-video-using-opencv/ --> for 1 (data preparation)\n",
        "\n",
        "https://www.kaggle.com/code/nityampareek/using-deepsort-object-tracker-with-yolov5#DeepSORT --> for 2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc750a3",
      "metadata": {
        "id": "5cc750a3"
      },
      "source": [
        "## `0` Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef00183",
      "metadata": {
        "id": "9ef00183"
      },
      "source": [
        "Run this in a terminal first\n",
        "\n",
        "*python3 -m venv venv && source venv/bin/activate && python -m pip install --upgrade pip && python -m pip install -r requirements.txt*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "GRf6EEWCgw_j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GRf6EEWCgw_j",
        "outputId": "c252b573-d9a3-4dff-d298-beb7f34d83b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.3.229)\n",
            "Requirement already satisfied: numpy<=2.3.4,>=1.23.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.2.6)\n",
            "Requirement already satisfied: matplotlib<=3.10.7,>=3.3.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (3.10.7)\n",
            "Requirement already satisfied: opencv-python<=4.12.0.88,>=4.6.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow<=12.0.0,>=7.1.2 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml<=6.0.3,>=5.3.1 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests<=2.32.5,>=2.23.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.32.5)\n",
            "Requirement already satisfied: scipy<=1.16.3,>=1.4.1 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch!=2.4.0,<=2.9.1,>=1.8.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.9.1)\n",
            "Requirement already satisfied: torchvision<=0.24.1,>=0.9.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (0.24.1)\n",
            "Requirement already satisfied: psutil<=7.1.3,>=5.8.0 in c:\\users\\ana gordon\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (7.1.3)\n",
            "Requirement already satisfied: polars<=1.35.2,>=0.20.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (1.35.2)\n",
            "Requirement already satisfied: ultralytics-thop<=2.0.18 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: polars-runtime-32==1.35.2 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from polars<=1.35.2,>=0.20.0->ultralytics) (1.35.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<=2.32.5,>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<=2.32.5,>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<=2.32.5,>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<=2.32.5,>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=2.4.0,<=2.9.1,>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=2.4.0,<=2.9.1,>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=2.4.0,<=2.9.1,>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=2.4.0,<=2.9.1,>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=2.4.0,<=2.9.1,>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=2.4.0,<=2.9.1,>=1.8.0->ultralytics) (2025.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=2.4.0,<=2.9.1,>=1.8.0->ultralytics) (70.2.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<=3.10.7,>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch!=2.4.0,<=2.9.1,>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch!=2.4.0,<=2.9.1,>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: opencv-python in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opencv-python) (2.2.6)\n",
            "Requirement already satisfied: opencv-python in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opencv-python) (2.2.6)\n",
            "Requirement already satisfied: deep-sort-realtime in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deep-sort-realtime) (2.2.6)\n",
            "Requirement already satisfied: scipy in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deep-sort-realtime) (1.16.3)\n",
            "Requirement already satisfied: opencv-python in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deep-sort-realtime) (4.12.0.88)\n",
            "Requirement already satisfied: deep-sort-realtime in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deep-sort-realtime) (2.2.6)\n",
            "Requirement already satisfied: scipy in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deep-sort-realtime) (1.16.3)\n",
            "Requirement already satisfied: opencv-python in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deep-sort-realtime) (4.12.0.88)\n",
            "Requirement already satisfied: datetime in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.0)\n",
            "Requirement already satisfied: zope.interface in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datetime) (8.1.1)\n",
            "Requirement already satisfied: pytz in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datetime) (2025.2)\n",
            "Requirement already satisfied: datetime in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.0)\n",
            "Requirement already satisfied: zope.interface in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datetime) (8.1.1)\n",
            "Requirement already satisfied: pytz in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datetime) (2025.2)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.24.1)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.10.0.dev20251124+cu128)\n",
            "Requirement already satisfied: filelock in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.24.1)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.10.0.dev20251124+cu128)\n",
            "Requirement already satisfied: filelock in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ana gordon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install opencv-python\n",
        "!pip install deep-sort-realtime\n",
        "!pip install datetime\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3db707b",
      "metadata": {
        "id": "d3db707b"
      },
      "source": [
        "### `Imports`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "73f8905c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment vars BEFORE importing torch/CUDA initialization\n",
        "import os\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c29b52d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c29b52d1",
        "outputId": "b1890bd1-5c1e-4ef9-bebc-5000adb34c1d"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import glob\n",
        "import torchvision\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd87807",
      "metadata": {
        "id": "abd87807"
      },
      "source": [
        "### `Paths`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "11266573",
      "metadata": {
        "id": "11266573"
      },
      "outputs": [],
      "source": [
        "data_path = './Object_Tracking/'\n",
        "\n",
        "task1_images_path = os.path.join(data_path, 'Task1/Images/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb8acba",
      "metadata": {
        "id": "abb8acba"
      },
      "source": [
        "## `1` Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "daebe2ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video saved to: task1_input.mp4\n"
          ]
        }
      ],
      "source": [
        "#1.1 convert images to video and 1.2 save output video\n",
        "input_video = 'task1_input.mp4'\n",
        "\n",
        "def get_video_writer(frame_size, filename):\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(filename, fourcc, 14, frame_size)\n",
        "    return writer\n",
        "\n",
        "def create_input_video(source, filename):\n",
        "    if isinstance(source, str) and os.path.isdir(source):\n",
        "        images = sorted([p for p in os.listdir(source) if p.endswith('.jpg')])\n",
        "        first = cv2.imread(os.path.join(source, images[0]))\n",
        "        h, w = first.shape[:2]\n",
        "        writer = get_video_writer((w, h), filename)\n",
        "        for img_name in images:\n",
        "            img = cv2.imread(os.path.join(source, img_name))\n",
        "            writer.write(img)\n",
        "        writer.release()\n",
        "        return filename\n",
        "        \n",
        "create_input_video(task1_images_path, input_video)\n",
        "print(f\"Video saved to: {input_video}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c09b708",
      "metadata": {},
      "source": [
        "## `2` Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a0e79260",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 21.6ms\n",
            "0: 384x640 16 persons, 1 umbrella, 21.6ms\n",
            "Speed: 1.3ms preprocess, 21.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 21.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.4ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.4ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 16.8ms\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 16.8ms\n",
            "Speed: 1.1ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 18.5ms\n",
            "Speed: 1.4ms preprocess, 18.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 umbrella, 18.5ms\n",
            "Speed: 1.4ms preprocess, 18.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 24.3ms\n",
            "Speed: 2.1ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 24.3ms\n",
            "Speed: 2.1ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 15.8ms\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 15.8ms\n",
            "Speed: 1.0ms preprocess, 15.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 15.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 horse, 1 umbrella, 18.7ms\n",
            "Speed: 1.4ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 horse, 1 umbrella, 18.7ms\n",
            "Speed: 1.4ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 horse, 1 umbrella, 18.1ms\n",
            "\n",
            "0: 384x640 18 persons, 1 horse, 1 umbrella, 18.1ms\n",
            "Speed: 1.4ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 14 persons, 1 umbrella, 17.3ms\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 14 persons, 1 umbrella, 17.3ms\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 14 persons, 1 umbrella, 19.9ms\n",
            "Speed: 1.5ms preprocess, 19.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 14 persons, 1 umbrella, 19.9ms\n",
            "Speed: 1.5ms preprocess, 19.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 19.6ms\n",
            "Speed: 1.5ms preprocess, 19.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 19.6ms\n",
            "Speed: 1.5ms preprocess, 19.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1 umbrella, 1 handbag, 16.7ms\n",
            "\n",
            "0: 384x640 14 persons, 1 umbrella, 1 handbag, 16.7ms\n",
            "Speed: 1.4ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 19.3ms\n",
            "Speed: 1.2ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 19.3ms\n",
            "Speed: 1.2ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1 umbrella, 18.8ms\n",
            "\n",
            "0: 384x640 14 persons, 1 umbrella, 18.8ms\n",
            "Speed: 1.1ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 19.2ms\n",
            "Speed: 1.5ms preprocess, 19.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 19.2ms\n",
            "Speed: 1.5ms preprocess, 19.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 handbag, 18.5ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 handbag, 18.5ms\n",
            "Speed: 1.3ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 1 handbag, 19.1ms\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 1 handbag, 19.1ms\n",
            "Speed: 1.4ms preprocess, 19.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 19.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.6ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 18.6ms\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 handbag, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 handbag, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 18.5ms\n",
            "Speed: 1.5ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 18.5ms\n",
            "Speed: 1.5ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 17.4ms\n",
            "Speed: 1.4ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 17.4ms\n",
            "Speed: 1.4ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 18.1ms\n",
            "Speed: 1.0ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 18.1ms\n",
            "Speed: 1.0ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 18.1ms\n",
            "\n",
            "0: 384x640 17 persons, 18.1ms\n",
            "Speed: 1.1ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 18.1ms\n",
            "Speed: 1.0ms preprocess, 18.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 18.1ms\n",
            "Speed: 1.0ms preprocess, 18.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 18.2ms\n",
            "Speed: 1.1ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 18.2ms\n",
            "Speed: 1.1ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 18.0ms\n",
            "Speed: 1.4ms preprocess, 18.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 18.0ms\n",
            "Speed: 1.4ms preprocess, 18.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 18.2ms\n",
            "Speed: 2.3ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 18.2ms\n",
            "Speed: 2.3ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 18.3ms\n",
            "Speed: 1.4ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 18.3ms\n",
            "Speed: 1.4ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 handbag, 18.4ms\n",
            "\n",
            "0: 384x640 19 persons, 1 handbag, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 17.2ms\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 18.5ms\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 18.5ms\n",
            "Speed: 1.4ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 19.6ms\n",
            "Speed: 1.0ms preprocess, 19.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 handbag, 19.6ms\n",
            "Speed: 1.0ms preprocess, 19.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 17.7ms\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 18.9ms\n",
            "Speed: 1.5ms preprocess, 18.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 handbag, 18.9ms\n",
            "Speed: 1.5ms preprocess, 18.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 18.9ms\n",
            "Speed: 1.3ms preprocess, 18.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 handbag, 18.9ms\n",
            "Speed: 1.3ms preprocess, 18.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 18.3ms\n",
            "Speed: 1.1ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 18.3ms\n",
            "Speed: 1.1ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 14 persons, 1 umbrella, 1 handbag, 19.2ms\n",
            "Speed: 1.4ms preprocess, 19.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 14 persons, 1 umbrella, 1 handbag, 19.2ms\n",
            "Speed: 1.4ms preprocess, 19.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 18.8ms\n",
            "Speed: 1.3ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 18.8ms\n",
            "Speed: 1.3ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 handbag, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 handbag, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 2 handbags, 18.2ms\n",
            "Speed: 1.0ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 2 handbags, 18.2ms\n",
            "Speed: 1.0ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 2 handbags, 18.2ms\n",
            "\n",
            "0: 384x640 18 persons, 2 handbags, 18.2ms\n",
            "Speed: 1.0ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 17.4ms\n",
            "Speed: 1.2ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 handbag, 17.4ms\n",
            "Speed: 1.2ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 18.3ms\n",
            "Speed: 1.3ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 handbag, 18.3ms\n",
            "Speed: 1.3ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 19.7ms\n",
            "Speed: 1.4ms preprocess, 19.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 19.7ms\n",
            "Speed: 1.4ms preprocess, 19.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 17.9ms\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 17.2ms\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 umbrella, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 19.0ms\n",
            "Speed: 1.0ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 19.0ms\n",
            "Speed: 1.0ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 umbrella, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 16.6ms\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 16.6ms\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 17.6ms\n",
            "Speed: 1.3ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 17.6ms\n",
            "Speed: 1.3ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 18.1ms\n",
            "Speed: 1.3ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 18.1ms\n",
            "Speed: 1.3ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 16.8ms\n",
            "Speed: 1.3ms preprocess, 16.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 16.8ms\n",
            "Speed: 1.3ms preprocess, 16.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 18.7ms\n",
            "\n",
            "0: 384x640 17 persons, 18.7ms\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 17.6ms\n",
            "Speed: 1.0ms preprocess, 17.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 17.6ms\n",
            "Speed: 1.0ms preprocess, 17.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 19.3ms\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 19.3ms\n",
            "Speed: 1.3ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 16.6ms\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 16.6ms\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.0ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.0ms\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 18.8ms\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 18.8ms\n",
            "Speed: 1.4ms preprocess, 18.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 18.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 16.7ms\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.0ms\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 18.0ms\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.3ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.3ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 tv, 19.3ms\n",
            "Speed: 1.0ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 tv, 19.3ms\n",
            "Speed: 1.0ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 2 tvs, 21.0ms\n",
            "Speed: 1.1ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 2 tvs, 21.0ms\n",
            "Speed: 1.1ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 tv, 18.2ms\n",
            "Speed: 1.4ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 tv, 18.2ms\n",
            "Speed: 1.4ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 handbag, 1 tv, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 handbag, 1 tv, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 tv, 18.7ms\n",
            "Speed: 1.3ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 tv, 18.7ms\n",
            "Speed: 1.3ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 tv, 17.8ms\n",
            "\n",
            "0: 384x640 18 persons, 1 tv, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 tv, 17.7ms\n",
            "\n",
            "0: 384x640 17 persons, 1 tv, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 tv, 18.4ms\n",
            "Speed: 1.3ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 tv, 18.4ms\n",
            "Speed: 1.3ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 1 tv, 20.2ms\n",
            "Speed: 1.0ms preprocess, 20.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 handbag, 1 tv, 20.2ms\n",
            "Speed: 1.0ms preprocess, 20.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 1 tv, 18.6ms\n",
            "Speed: 1.0ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 handbag, 1 tv, 18.6ms\n",
            "Speed: 1.0ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 2 handbags, 1 tv, 18.0ms\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 2 handbags, 1 tv, 18.0ms\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 1 tv, 1 clock, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 handbag, 1 tv, 1 clock, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 2 tvs, 1 clock, 18.5ms\n",
            "Speed: 1.5ms preprocess, 18.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 handbag, 2 tvs, 1 clock, 18.5ms\n",
            "Speed: 1.5ms preprocess, 18.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 2 handbags, 1 tv, 17.9ms\n",
            "\n",
            "0: 384x640 17 persons, 2 handbags, 1 tv, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 handbag, 1 tv, 18.8ms\n",
            "\n",
            "0: 384x640 19 persons, 1 handbag, 1 tv, 18.8ms\n",
            "Speed: 1.0ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 14.1ms\n",
            "Speed: 1.2ms preprocess, 14.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 14.1ms\n",
            "Speed: 1.2ms preprocess, 14.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 19.3ms\n",
            "Speed: 1.3ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 19.3ms\n",
            "Speed: 1.3ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 handbag, 18.9ms\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 handbag, 18.9ms\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 handbag, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 handbag, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 17.9ms\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 17.5ms\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 17.3ms\n",
            "Speed: 1.4ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 handbag, 17.3ms\n",
            "Speed: 1.4ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 14 persons, 1 handbag, 18.1ms\n",
            "Speed: 1.0ms preprocess, 18.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 14 persons, 1 handbag, 18.1ms\n",
            "Speed: 1.0ms preprocess, 18.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 18.7ms\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 18.7ms\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 1 tv, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 handbag, 1 tv, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 1 tv, 17.4ms\n",
            "Speed: 1.1ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 handbag, 1 tv, 17.4ms\n",
            "Speed: 1.1ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 handbag, 2 tvs, 18.5ms\n",
            "Speed: 1.4ms preprocess, 18.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 handbag, 2 tvs, 18.5ms\n",
            "Speed: 1.4ms preprocess, 18.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 tv, 18.7ms\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 tv, 18.7ms\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 2 tvs, 20.3ms\n",
            "Speed: 1.3ms preprocess, 20.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 2 tvs, 20.3ms\n",
            "Speed: 1.3ms preprocess, 20.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 14 persons, 4 handbags, 2 tvs, 18.8ms\n",
            "Speed: 1.5ms preprocess, 18.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 14 persons, 4 handbags, 2 tvs, 18.8ms\n",
            "Speed: 1.5ms preprocess, 18.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 12 persons, 4 handbags, 2 tvs, 19.3ms\n",
            "Speed: 1.0ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 12 persons, 4 handbags, 2 tvs, 19.3ms\n",
            "Speed: 1.0ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 12 persons, 2 handbags, 2 tvs, 18.9ms\n",
            "Speed: 1.3ms preprocess, 18.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 12 persons, 2 handbags, 2 tvs, 18.9ms\n",
            "Speed: 1.3ms preprocess, 18.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 14 persons, 1 handbag, 1 tv, 20.0ms\n",
            "Speed: 1.0ms preprocess, 20.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 14 persons, 1 handbag, 1 tv, 20.0ms\n",
            "Speed: 1.0ms preprocess, 20.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 12 persons, 1 handbag, 1 tv, 18.7ms\n",
            "Speed: 1.5ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 12 persons, 1 handbag, 1 tv, 18.7ms\n",
            "Speed: 1.5ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1 handbag, 19.2ms\n",
            "\n",
            "0: 384x640 14 persons, 1 handbag, 19.2ms\n",
            "Speed: 1.3ms preprocess, 19.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 19.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 13 persons, 1 handbag, 1 tv, 19.1ms\n",
            "Speed: 1.0ms preprocess, 19.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 13 persons, 1 handbag, 1 tv, 19.1ms\n",
            "Speed: 1.0ms preprocess, 19.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 2 handbags, 1 tv, 16.9ms\n",
            "\n",
            "0: 384x640 14 persons, 2 handbags, 1 tv, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 1 tv, 15.7ms\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 1 tv, 15.7ms\n",
            "Speed: 1.4ms preprocess, 15.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 15.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 16.8ms\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 1 tv, 18.4ms\n",
            "Speed: 1.1ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 handbag, 1 tv, 18.4ms\n",
            "Speed: 1.1ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 1 tv, 18.7ms\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 1 tv, 18.7ms\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 1 tv, 18.6ms\n",
            "Speed: 1.4ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 handbag, 1 tv, 18.6ms\n",
            "Speed: 1.4ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 2 tvs, 17.0ms\n",
            "Speed: 1.1ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 handbag, 2 tvs, 17.0ms\n",
            "Speed: 1.1ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 2 handbags, 2 tvs, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 2 handbags, 2 tvs, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 handbag, 1 tv, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 handbag, 1 tv, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 backpack, 1 handbag, 1 tv, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 backpack, 1 handbag, 1 tv, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 backpack, 1 handbag, 1 tv, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 backpack, 1 handbag, 1 tv, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1 backpack, 1 handbag, 1 tv, 17.1ms\n",
            "\n",
            "0: 384x640 16 persons, 1 backpack, 1 handbag, 1 tv, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 backpack, 1 handbag, 1 tv, 17.0ms\n",
            "Speed: 1.4ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 backpack, 1 handbag, 1 tv, 17.0ms\n",
            "Speed: 1.4ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 backpack, 1 handbag, 1 tv, 17.4ms\n",
            "Speed: 1.1ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 backpack, 1 handbag, 1 tv, 17.4ms\n",
            "Speed: 1.1ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 1 tv, 17.2ms\n",
            "Speed: 1.4ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 handbag, 1 tv, 17.2ms\n",
            "Speed: 1.4ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 2 tvs, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 handbag, 2 tvs, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 2 handbags, 2 tvs, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 umbrella, 2 handbags, 2 tvs, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 2 handbags, 17.6ms\n",
            "Speed: 1.1ms preprocess, 17.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 umbrella, 2 handbags, 17.6ms\n",
            "Speed: 1.1ms preprocess, 17.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 2 handbags, 14.8ms\n",
            "\n",
            "0: 384x640 19 persons, 2 handbags, 14.8ms\n",
            "Speed: 1.0ms preprocess, 14.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 14.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 2 handbags, 2 tvs, 17.0ms\n",
            "\n",
            "0: 384x640 17 persons, 2 handbags, 2 tvs, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 4 handbags, 2 tvs, 18.6ms\n",
            "Speed: 1.3ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 4 handbags, 2 tvs, 18.6ms\n",
            "Speed: 1.3ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 handbag, 1 tv, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 handbag, 1 tv, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1 handbag, 2 tvs, 16.5ms\n",
            "\n",
            "0: 384x640 13 persons, 1 handbag, 2 tvs, 16.5ms\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 3 tvs, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 3 tvs, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 2 tvs, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 umbrella, 1 handbag, 2 tvs, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 1 tv, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 1 tv, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 handbag, 1 tv, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 1 handbag, 1 tv, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 1 tv, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 1 tv, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 handbag, 1 tv, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 1 handbag, 1 tv, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 1 handbag, 1 tv, 17.8ms\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 1 handbag, 1 tv, 17.8ms\n",
            "Speed: 1.4ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 1 tv, 17.4ms\n",
            "Speed: 1.3ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 1 tv, 17.4ms\n",
            "Speed: 1.3ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 14 persons, 1 umbrella, 1 handbag, 1 tv, 17.5ms\n",
            "Speed: 1.1ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 14 persons, 1 umbrella, 1 handbag, 1 tv, 17.5ms\n",
            "Speed: 1.1ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 14 persons, 1 umbrella, 1 handbag, 1 tv, 17.3ms\n",
            "Speed: 1.0ms preprocess, 17.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 14 persons, 1 umbrella, 1 handbag, 1 tv, 17.3ms\n",
            "Speed: 1.0ms preprocess, 17.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 2 handbags, 1 tv, 16.0ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 2 handbags, 1 tv, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 2 handbags, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 umbrella, 2 handbags, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 2 handbags, 18.1ms\n",
            "Speed: 1.4ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 umbrella, 2 handbags, 18.1ms\n",
            "Speed: 1.4ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 2 handbags, 1 tv, 16.6ms\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 2 handbags, 1 tv, 16.6ms\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 18.2ms\n",
            "Speed: 1.0ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 umbrella, 1 handbag, 18.2ms\n",
            "Speed: 1.0ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 1 handbag, 17.7ms\n",
            "Speed: 1.5ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 umbrella, 1 handbag, 17.7ms\n",
            "Speed: 1.5ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 1 handbag, 1 tv, 17.8ms\n",
            "Speed: 1.2ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 umbrella, 1 handbag, 1 tv, 17.8ms\n",
            "Speed: 1.2ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 2 handbags, 1 tv, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 2 handbags, 1 tv, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 1 handbag, 1 tv, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 1 handbag, 1 tv, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 1 tv, 16.6ms\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 handbag, 1 tv, 16.6ms\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 18.0ms\n",
            "Speed: 1.4ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 umbrella, 18.0ms\n",
            "Speed: 1.4ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 tv, 16.7ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 tv, 16.7ms\n",
            "Speed: 1.3ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 handbag, 17.4ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 handbag, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.3ms\n",
            "Speed: 1.5ms preprocess, 18.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 18.3ms\n",
            "Speed: 1.5ms preprocess, 18.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 tv, 18.1ms\n",
            "Speed: 1.0ms preprocess, 18.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 1 tv, 18.1ms\n",
            "Speed: 1.0ms preprocess, 18.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 horse, 1 umbrella, 17.4ms\n",
            "Speed: 1.1ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 horse, 1 umbrella, 17.4ms\n",
            "Speed: 1.1ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 17.1ms\n",
            "Speed: 1.2ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 17.1ms\n",
            "Speed: 1.2ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 16.7ms\n",
            "Speed: 1.1ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 16.7ms\n",
            "Speed: 1.1ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 17.2ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 1 umbrella, 18.0ms\n",
            "Speed: 1.1ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 1 umbrella, 18.0ms\n",
            "Speed: 1.1ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 1 umbrella, 18.4ms\n",
            "Speed: 1.3ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 1 umbrella, 18.4ms\n",
            "Speed: 1.3ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 23 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.0ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 23 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.0ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 1 umbrella, 18.7ms\n",
            "Speed: 1.4ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 1 umbrella, 18.7ms\n",
            "Speed: 1.4ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 18.7ms\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 18.7ms\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 17.8ms\n",
            "Speed: 1.0ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 17.8ms\n",
            "Speed: 1.0ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 umbrella, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 14 persons, 21.1ms\n",
            "Speed: 1.0ms preprocess, 21.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 14 persons, 21.1ms\n",
            "Speed: 1.0ms preprocess, 21.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 19.3ms\n",
            "Speed: 1.4ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 1 umbrella, 19.3ms\n",
            "Speed: 1.4ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.4ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.4ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 19.5ms\n",
            "Speed: 1.0ms preprocess, 19.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 19.5ms\n",
            "Speed: 1.0ms preprocess, 19.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 17.1ms\n",
            "\n",
            "0: 384x640 17 persons, 1 umbrella, 17.1ms\n",
            "Speed: 1.1ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 17.7ms\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 19.9ms\n",
            "Speed: 1.4ms preprocess, 19.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 19.9ms\n",
            "Speed: 1.4ms preprocess, 19.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 17.6ms\n",
            "Speed: 1.1ms preprocess, 17.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 17.6ms\n",
            "Speed: 1.1ms preprocess, 17.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.6ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 suitcase, 16.5ms\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 suitcase, 16.5ms\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 15.4ms\n",
            "Speed: 1.0ms preprocess, 15.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 15.4ms\n",
            "Speed: 1.0ms preprocess, 15.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.6ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 18.3ms\n",
            "Speed: 1.3ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 18.3ms\n",
            "Speed: 1.3ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 17.6ms\n",
            "Speed: 1.0ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 17.6ms\n",
            "Speed: 1.0ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 17.3ms\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 17.3ms\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 18.1ms\n",
            "Speed: 1.1ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 umbrella, 18.1ms\n",
            "Speed: 1.1ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 tv, 17.1ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 tv, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 tv, 17.7ms\n",
            "\n",
            "0: 384x640 18 persons, 1 umbrella, 1 tv, 17.7ms\n",
            "Speed: 1.1ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 18.9ms\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 18.9ms\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 19.2ms\n",
            "Speed: 1.0ms preprocess, 19.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 19.2ms\n",
            "Speed: 1.0ms preprocess, 19.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 1 handbag, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 1 handbag, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 1 umbrella, 1 handbag, 18.1ms\n",
            "Speed: 1.1ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 1 umbrella, 1 handbag, 18.1ms\n",
            "Speed: 1.1ms preprocess, 18.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 1 tv, 16.7ms\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 1 tv, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 18.7ms\n",
            "Speed: 1.7ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 18.7ms\n",
            "Speed: 1.7ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 tv, 19.2ms\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 1 tv, 19.2ms\n",
            "Speed: 1.4ms preprocess, 19.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 19.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 18.5ms\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 18.5ms\n",
            "Speed: 1.1ms preprocess, 18.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 18.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 16.8ms\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 umbrella, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 umbrella, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 2 handbags, 17.8ms\n",
            "Speed: 1.4ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 15 persons, 1 umbrella, 2 handbags, 17.8ms\n",
            "Speed: 1.4ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 16 persons, 1 umbrella, 18.3ms\n",
            "Speed: 1.1ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 16 persons, 1 umbrella, 18.3ms\n",
            "Speed: 1.1ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.4ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 umbrella, 18.6ms\n",
            "Speed: 1.4ms preprocess, 18.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 1 umbrella, 17.6ms\n",
            "Speed: 1.0ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 1 umbrella, 17.6ms\n",
            "Speed: 1.0ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 1 tv, 17.9ms\n",
            "Speed: 1.1ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 1 tv, 17.9ms\n",
            "Speed: 1.1ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 18.6ms\n",
            "Speed: 1.0ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 18.6ms\n",
            "Speed: 1.0ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 1 handbag, 2 tvs, 17.3ms\n",
            "\n",
            "0: 384x640 21 persons, 1 handbag, 2 tvs, 17.3ms\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.8ms\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.9ms\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 20.9ms\n",
            "Speed: 1.4ms preprocess, 20.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 2 tvs, 20.9ms\n",
            "Speed: 1.4ms preprocess, 20.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 2 tvs, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 18.8ms\n",
            "Speed: 1.1ms preprocess, 18.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 2 tvs, 18.8ms\n",
            "Speed: 1.1ms preprocess, 18.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 1 backpack, 1 tv, 16.7ms\n",
            "Speed: 1.1ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 1 backpack, 1 tv, 16.7ms\n",
            "Speed: 1.1ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 1 tv, 17.3ms\n",
            "Speed: 1.5ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 1 tv, 17.3ms\n",
            "Speed: 1.5ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 16.6ms\n",
            "Speed: 1.4ms preprocess, 16.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 2 tvs, 16.6ms\n",
            "Speed: 1.4ms preprocess, 16.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.5ms\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.5ms\n",
            "Speed: 1.3ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.5ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.5ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 18.0ms\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 18.0ms\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 tvs, 17.6ms\n",
            "\n",
            "0: 384x640 23 persons, 2 tvs, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 tvs, 17.4ms\n",
            "\n",
            "0: 384x640 23 persons, 2 tvs, 17.4ms\n",
            "Speed: 1.2ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.2ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 25 persons, 1 handbag, 2 tvs, 18.1ms\n",
            "Speed: 1.4ms preprocess, 18.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 25 persons, 1 handbag, 2 tvs, 18.1ms\n",
            "Speed: 1.4ms preprocess, 18.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 19.2ms\n",
            "Speed: 1.3ms preprocess, 19.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 2 tvs, 19.2ms\n",
            "Speed: 1.3ms preprocess, 19.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 17.9ms\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.1ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 23 persons, 2 tvs, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 23 persons, 2 tvs, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 17.8ms\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 17.9ms\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 1 tv, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 1 tv, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 18.4ms\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 23 persons, 1 tv, 18.4ms\n",
            "Speed: 1.4ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 23 persons, 1 tv, 18.4ms\n",
            "Speed: 1.4ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 tv, 17.8ms\n",
            "\n",
            "0: 384x640 25 persons, 1 tv, 17.8ms\n",
            "Speed: 1.3ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 1 tv, 18.2ms\n",
            "\n",
            "0: 384x640 22 persons, 1 tv, 18.2ms\n",
            "Speed: 1.1ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 tv, 18.8ms\n",
            "\n",
            "0: 384x640 24 persons, 1 tv, 18.8ms\n",
            "Speed: 1.3ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 23 persons, 1 tv, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 23 persons, 1 tv, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 26 persons, 2 tvs, 18.3ms\n",
            "Speed: 1.2ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 26 persons, 2 tvs, 18.3ms\n",
            "Speed: 1.2ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 2 tvs, 18.4ms\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 2 tvs, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.1ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 24 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.1ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 2 tvs, 20.4ms\n",
            "Speed: 1.0ms preprocess, 20.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 26 persons, 1 handbag, 2 tvs, 20.4ms\n",
            "Speed: 1.0ms preprocess, 20.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 19.3ms\n",
            "Speed: 1.2ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 2 tvs, 19.3ms\n",
            "Speed: 1.2ms preprocess, 19.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 2 tvs, 18.8ms\n",
            "\n",
            "0: 384x640 17 persons, 2 tvs, 18.8ms\n",
            "Speed: 1.3ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 18.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 2 tvs, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 19.5ms\n",
            "Speed: 0.9ms preprocess, 19.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 24 persons, 2 tvs, 19.5ms\n",
            "Speed: 0.9ms preprocess, 19.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 27 persons, 2 tvs, 19.6ms\n",
            "Speed: 1.1ms preprocess, 19.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 27 persons, 2 tvs, 19.6ms\n",
            "Speed: 1.1ms preprocess, 19.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 19.0ms\n",
            "Speed: 1.0ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 2 tvs, 19.0ms\n",
            "Speed: 1.0ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 19.5ms\n",
            "Speed: 1.0ms preprocess, 19.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 2 tvs, 19.5ms\n",
            "Speed: 1.0ms preprocess, 19.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.3ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.3ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 tv, 17.6ms\n",
            "Speed: 1.3ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 tv, 17.6ms\n",
            "Speed: 1.3ms preprocess, 17.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 tv, 17.9ms\n",
            "\n",
            "0: 384x640 20 persons, 1 tv, 17.9ms\n",
            "Speed: 0.9ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 0.9ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 18.7ms\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.4ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 18.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 2 tvs, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.1ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.1ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 16.5ms\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 2 tvs, 16.5ms\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 2 tvs, 17.7ms\n",
            "\n",
            "0: 384x640 18 persons, 2 tvs, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 17 persons, 2 tvs, 18.8ms\n",
            "Speed: 1.3ms preprocess, 18.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 17 persons, 2 tvs, 18.8ms\n",
            "Speed: 1.3ms preprocess, 18.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 18 persons, 2 tvs, 19.0ms\n",
            "Speed: 1.4ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 18 persons, 2 tvs, 19.0ms\n",
            "Speed: 1.4ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.4ms\n",
            "Speed: 1.4ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 2 tvs, 17.4ms\n",
            "Speed: 1.4ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.1ms\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 17.3ms\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 17.3ms\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 17.5ms\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 tv, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 tv, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 1 tv, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 1 tv, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 25 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.2ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 25 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.2ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 tv, 19.0ms\n",
            "\n",
            "0: 384x640 19 persons, 1 tv, 19.0ms\n",
            "Speed: 1.0ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 19 persons, 1 tv, 19.6ms\n",
            "Speed: 1.3ms preprocess, 19.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 19 persons, 1 tv, 19.6ms\n",
            "Speed: 1.3ms preprocess, 19.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 1 tv, 18.5ms\n",
            "Speed: 1.4ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 1 tv, 18.5ms\n",
            "Speed: 1.4ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.4ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.4ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 18.9ms\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 18.9ms\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 16.8ms\n",
            "Speed: 1.1ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 24 persons, 2 tvs, 16.8ms\n",
            "Speed: 1.1ms preprocess, 16.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 16.6ms\n",
            "Speed: 1.4ms preprocess, 16.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 2 tvs, 16.6ms\n",
            "Speed: 1.4ms preprocess, 16.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 25 persons, 1 tv, 24.0ms\n",
            "Speed: 1.0ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 25 persons, 1 tv, 24.0ms\n",
            "Speed: 1.0ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 23 persons, 1 tv, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 23 persons, 1 tv, 17.8ms\n",
            "Speed: 1.1ms preprocess, 17.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 17.1ms\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 17.1ms\n",
            "Speed: 1.3ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 17.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 18.4ms\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 18.4ms\n",
            "Speed: 1.4ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 17.2ms\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 17.2ms\n",
            "Speed: 1.1ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 17.2ms\n",
            "Speed: 1.1ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 24 persons, 2 tvs, 17.2ms\n",
            "Speed: 1.1ms preprocess, 17.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 25 persons, 2 tvs, 18.3ms\n",
            "Speed: 1.3ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 25 persons, 2 tvs, 18.3ms\n",
            "Speed: 1.3ms preprocess, 18.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 tvs, 19.0ms\n",
            "\n",
            "0: 384x640 25 persons, 2 tvs, 19.0ms\n",
            "Speed: 1.4ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.4ms preprocess, 19.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 17.5ms\n",
            "Speed: 1.1ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 24 persons, 2 tvs, 17.5ms\n",
            "Speed: 1.1ms preprocess, 17.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 tvs, 18.3ms\n",
            "\n",
            "0: 384x640 25 persons, 2 tvs, 18.3ms\n",
            "Speed: 0.9ms preprocess, 18.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 0.9ms preprocess, 18.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 17.3ms\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 2 tvs, 17.3ms\n",
            "Speed: 1.1ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.1ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.1ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 19.8ms\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 19.8ms\n",
            "Speed: 1.1ms preprocess, 19.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.1ms preprocess, 19.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.1ms preprocess, 18.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.1ms preprocess, 18.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 17.3ms\n",
            "Speed: 1.0ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 2 tvs, 17.3ms\n",
            "Speed: 1.0ms preprocess, 17.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 20 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 20 persons, 2 tvs, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 18.4ms\n",
            "\n",
            "0: 384x640 19 persons, 2 tvs, 18.4ms\n",
            "Speed: 1.3ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 16.7ms\n",
            "\n",
            "0: 384x640 22 persons, 2 tvs, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 22 persons, 3 tvs, 18.2ms\n",
            "Speed: 1.4ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 22 persons, 3 tvs, 18.2ms\n",
            "Speed: 1.4ms preprocess, 18.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 18.5ms\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 18.5ms\n",
            "Speed: 1.3ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.3ms preprocess, 18.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 17.4ms\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 21 persons, 2 tvs, 18.9ms\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 21 persons, 2 tvs, 18.9ms\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 24 persons, 3 tvs, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 24 persons, 3 tvs, 18.6ms\n",
            "Speed: 1.1ms preprocess, 18.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 25 persons, 3 tvs, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 25 persons, 3 tvs, 18.4ms\n",
            "Speed: 1.0ms preprocess, 18.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 27 persons, 3 tvs, 17.8ms\n",
            "Speed: 1.0ms preprocess, 17.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 27 persons, 3 tvs, 17.8ms\n",
            "Speed: 1.0ms preprocess, 17.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 26 persons, 2 backpacks, 3 tvs, 18.3ms\n",
            "Speed: 1.2ms preprocess, 18.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 26 persons, 2 backpacks, 3 tvs, 18.3ms\n",
            "Speed: 1.2ms preprocess, 18.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 24 persons, 2 tvs, 17.3ms\n",
            "Speed: 1.3ms preprocess, 17.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 24 persons, 2 tvs, 17.3ms\n",
            "Speed: 1.3ms preprocess, 17.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 25 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.1ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 25 persons, 2 tvs, 18.7ms\n",
            "Speed: 1.1ms preprocess, 18.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 25 persons, 1 backpack, 2 tvs, 17.8ms\n",
            "Speed: 1.0ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 25 persons, 1 backpack, 2 tvs, 17.8ms\n",
            "Speed: 1.0ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 2 tvs, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 24 persons, 1 handbag, 2 tvs, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 backpack, 1 handbag, 2 tvs, 17.8ms\n",
            "\n",
            "0: 384x640 25 persons, 1 backpack, 1 handbag, 2 tvs, 17.8ms\n",
            "Speed: 1.0ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Speed: 1.0ms preprocess, 17.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 2 tvs, 17.4ms\n",
            "Speed: 1.1ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "0: 384x640 26 persons, 1 backpack, 2 tvs, 17.4ms\n",
            "Speed: 1.1ms preprocess, 17.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m         detections.append([[xmin, ymin, width, height], \u001b[38;5;28mfloat\u001b[39m(conf), class_found])\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# update tracker\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m tracks = \u001b[43mtracker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 2.2 - draw bounding box and ID\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tr \u001b[38;5;129;01min\u001b[39;00m tracks:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deep_sort_realtime\\deepsort_tracker.py:199\u001b[39m, in \u001b[36mDeepSort.update_tracks\u001b[39m\u001b[34m(self, raw_detections, embeds, frame, today, others, instance_masks)\u001b[39m\n\u001b[32m    196\u001b[39m raw_detections = [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw_detections \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[32m0\u001b[39m][\u001b[32m2\u001b[39m] > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m d[\u001b[32m0\u001b[39m][\u001b[32m3\u001b[39m] > \u001b[32m0\u001b[39m]\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_embeds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_detections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstance_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Proper deep sort detection objects that consist of bbox, confidence and embedding.\u001b[39;00m\n\u001b[32m    202\u001b[39m detections = \u001b[38;5;28mself\u001b[39m.create_detections(raw_detections, embeds, instance_masks=instance_masks, others=others)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deep_sort_realtime\\deepsort_tracker.py:246\u001b[39m, in \u001b[36mDeepSort.generate_embeds\u001b[39m\u001b[34m(self, frame, raw_dets, instance_masks)\u001b[39m\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedder.predict(masked_crops)\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrops\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:135\u001b[39m, in \u001b[36mMobileNetv2_Embedder.predict\u001b[39m\u001b[34m(self, np_images)\u001b[39m\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.half:\n\u001b[32m    134\u001b[39m             this_batch = this_batch.half()\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     all_feats.extend(output.cpu().data.numpy())\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_feats\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deep_sort_realtime\\embedder\\mobilenetv2_bottle.py:117\u001b[39m, in \u001b[36mMobileNetV2_bottle.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     x = x.mean(\u001b[32m3\u001b[39m).mean(\u001b[32m2\u001b[39m)\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# x = self.classifier(x)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana Gordon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "input_video = globals().get('input_video', 'task1_input.mp4')\n",
        "output_video = 'task1.mp4'\n",
        "output_txt = 'task1.txt'\n",
        "\n",
        "# load model\n",
        "model = YOLO('yolov8n.pt')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CPU embedder for compatibility \n",
        "tracker = DeepSort(max_age=30, embedder_gpu=False) # 2.1 apply deepsort tracker\n",
        "\n",
        "cap = cv2.VideoCapture(input_video)\n",
        "frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "writer = get_video_writer((frame_w, frame_h), output_video)\n",
        "\n",
        "# tracking results\n",
        "f = open(output_txt, 'w')\n",
        "\n",
        "frame_number = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_number += 1\n",
        "\n",
        "    results = model(frame, device=device)\n",
        "    frame_result = results[0]  \n",
        "\n",
        "    detections = []\n",
        "\n",
        "    if hasattr(frame_result, 'boxes') and hasattr(frame_result.boxes, 'data') and frame_result.boxes.data is not None:\n",
        "        boxes = frame_result.boxes.data.cpu().numpy() if hasattr(frame_result.boxes.data, 'cpu') else frame_result.boxes.data.numpy()\n",
        "       \n",
        "        for b in boxes:\n",
        "            # bbox: [x1,y1,x2,y2,conf,class]\n",
        "            x1, y1, x2, y2, conf, class_found = (b.tolist() if hasattr(b, 'tolist') else list(b))\n",
        "            class_found = int(class_found)\n",
        "            \n",
        "            # keep only pedestrians (COCO class 0)\n",
        "            if class_found  != 0:\n",
        "                continue\n",
        "\n",
        "            xmin = int(max(0, round(x1)))\n",
        "            ymin = int(max(0, round(y1)))\n",
        "            width = int(round(x2 - x1))\n",
        "            height = int(round(y2 - y1))\n",
        "            detections.append([[xmin, ymin, width, height], float(conf), class_found])\n",
        "\n",
        "    # update tracker\n",
        "    tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "    # 2.2 - draw bounding box and ID\n",
        "    for tr in tracks:\n",
        "        if not tr.is_confirmed():\n",
        "            track_id = -1\n",
        "            continue\n",
        "        track_id = tr.track_id\n",
        "        left, top, right, bottom = tr.to_ltrb()\n",
        "        left, top, right, bottom  = int(left), int(top), int(right), int(bottom)\n",
        "        width = right-left\n",
        "        height = bottom-top\n",
        "        \n",
        "        # draw bbox and id\n",
        "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
        "        label = f'ID: {track_id}'\n",
        "        cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        \n",
        "        # results.txt: <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
        "        f.write(f'{frame_number},{track_id},{left},{top},{width},{height}\\n') # 2.4 writes to file task1.txt\n",
        "\n",
        "    writer.write(frame)\n",
        "\n",
        "# cleanup\n",
        "f.close()\n",
        "cap.release()\n",
        "writer.release() # 2.3 saves output video as task1.mp4\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2ff10a",
      "metadata": {},
      "source": [
        "## `3` Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "24fa10aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Tracking Metrics -----\n",
            "Frames evaluated: 429\n",
            "True Positives (TP): 6099\n",
            "Ground Truth boxes (GT): 26647\n",
            "False Positives (FP): 2003\n",
            "False Negatives (FN): 20548\n",
            "Identity Switches (IDSW): 158\n",
            "Precision: 0.7528\n",
            "Recall: 0.2289\n",
            "F1-score: 0.3510\n",
            "MOTA: 0.1478\n"
          ]
        }
      ],
      "source": [
        "# code in this section for model evaluation from Tutorial 12\n",
        "\n",
        "# IoU >= 0.5\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA['x'], boxB['x'])\n",
        "    yA = max(boxA['y'], boxB['y'])\n",
        "    xB = min(boxA['x'] + boxA['w'], boxB['x'] + boxB['w'])\n",
        "    yB = min(boxA['y'] + boxA['h'], boxB['y'] + boxB['h'])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = boxA['w'] * boxA['h']\n",
        "    boxBArea = boxB['w'] * boxB['h']\n",
        "    if (boxAArea + boxBArea - interArea) <= 0:\n",
        "        return 0\n",
        "    return interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "# Hungarian Algorithm or equivalent (aka Greedy matching)\n",
        "def greedy_match(gt_frame, pred_frame, iou_thresh=0.5):\n",
        "    matches = []\n",
        "    if len(gt_frame) == 0 or len(pred_frame) == 0:\n",
        "        return matches\n",
        "\n",
        "    # Compute IoU for all GT-Pred pairs on the same frame and only keep IoUs  threshold:\n",
        "    iou_matrix = []\n",
        "    for i, gt_row in gt_frame.iterrows():\n",
        "        for j, pred_row in pred_frame.iterrows():\n",
        "            score = iou(gt_row, pred_row)\n",
        "            if score >= iou_thresh:\n",
        "                iou_matrix.append((score, i, j))\n",
        "\n",
        "    # Sort descending by IoU\n",
        "    iou_matrix.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # greedy: You take the highest-IoU match first and assign it.\n",
        "    # Then you block (mark used) the GT and prediction so no one else can use them.\n",
        "\n",
        "    gt_used, pred_used = set(), set()\n",
        "\n",
        "    for score, i, j in iou_matrix:\n",
        "        if i not in gt_used and j not in pred_used:\n",
        "            matches.append((i, j, score))\n",
        "            gt_used.add(i)\n",
        "            pred_used.add(j)\n",
        "    return matches\n",
        "\n",
        "# track and count\n",
        "def compute_tracking_metrics(gt_file, pred_file, iou_thresh=0.5):\n",
        "    gt_df = pd.read_csv(\n",
        "        gt_file,\n",
        "        header=None,\n",
        "        usecols=[0,1,2,3,4,5],\n",
        "        names=['frame','id','x','y','w','h']\n",
        "    )\n",
        "\n",
        "    pred_df = pd.read_csv(\n",
        "        pred_file,\n",
        "        header=None,\n",
        "        usecols=[0,1,2,3,4,5],\n",
        "        names=['frame','id','x','y','w','h']\n",
        "    )\n",
        "\n",
        "    frames = sorted(gt_df['frame'].unique())\n",
        "\n",
        "    TP_total, FP_total, FN_total, IDSW_total = 0, 0, 0, 0\n",
        "    prev_assignment = {}      # GT id  Pred id from previous frame\n",
        "\n",
        "    for f in frames:\n",
        "        gt_frame = gt_df[gt_df['frame'] == f].reset_index(drop=True)\n",
        "        pred_frame = pred_df[pred_df['frame'] == f].reset_index(drop=True)\n",
        "\n",
        "        matches = greedy_match(gt_frame, pred_frame, iou_thresh)\n",
        "\n",
        "        # Count TP/FP/FN\n",
        "        TP_total += len(matches)\n",
        "        FP_total += len(pred_frame) - len(matches) # unmatched predictions\n",
        "        FN_total += len(gt_frame) - len(matches)   # unmatched GT\n",
        "\n",
        "        current_assignment = {}\n",
        "\n",
        "        for i, j, _ in matches:\n",
        "            gt_id = gt_frame.iloc[i]['id']\n",
        "            pred_id = pred_frame.iloc[j]['id']\n",
        "            current_assignment[gt_id] = pred_id\n",
        "\n",
        "            # Identity switch\n",
        "            # A GT id is tracked across frames.\n",
        "            # If the matched predicted ID changes  identity switch.\n",
        "\n",
        "            # Example:\n",
        "            # Frame 10: GT 5  Pred 12\n",
        "            # Frame 11: GT 5  Pred 19  identity switched\n",
        "\n",
        "            if gt_id in prev_assignment and prev_assignment[gt_id] != pred_id:\n",
        "                IDSW_total += 1\n",
        "\n",
        "\n",
        "        # Update previous assignment for next frame\n",
        "        prev_assignment = current_assignment\n",
        "\n",
        "    # Precision / Recall / F1\n",
        "    precision = TP_total / (TP_total + FP_total) if TP_total + FP_total > 0 else 0\n",
        "    recall = TP_total / (TP_total + FN_total) if TP_total + FN_total > 0 else 0\n",
        "    f1 = (2 * precision * recall / (precision + recall)\n",
        "          if precision + recall > 0 else 0)\n",
        "\n",
        "    # MOTA\n",
        "    GT_total = len(gt_df)  # total GT boxes\n",
        "    MOTA = 1 - (FN_total + FP_total + IDSW_total) / GT_total if GT_total > 0 else 0\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"----- Tracking Metrics -----\")\n",
        "    print(f\"Frames evaluated: {len(frames)}\")\n",
        "    print(f\"True Positives (TP): {TP_total}\")\n",
        "    print(f\"Ground Truth boxes (GT): {GT_total}\")\n",
        "    print(f\"False Positives (FP): {FP_total}\")\n",
        "    print(f\"False Negatives (FN): {FN_total}\")\n",
        "    print(f\"Identity Switches (IDSW): {IDSW_total}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(f\"MOTA: {MOTA:.4f}\")\n",
        "\n",
        "# print metrics\n",
        "compute_tracking_metrics(\n",
        "    gt_file=os.path.join(data_path, 'Task1/gt/gt.txt'),\n",
        "    pred_file=output_txt,\n",
        "    iou_thresh=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce3e412",
      "metadata": {},
      "source": [
        "## `4` Prediction & Kaggle Competition"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
